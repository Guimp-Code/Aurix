#!/bin/bash

echo "ğŸš€ Configurando Ollama NITRO para Aurix..."

# Verificar se Ollama estÃ¡ instalado
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama nÃ£o encontrado. Instalando..."
    curl -fsSL https://ollama.ai/install.sh | sh
else
    echo "âœ… Ollama jÃ¡ instalado"
fi

# Verificar modelos disponÃ­veis
echo "ğŸ“Š Modelos disponÃ­veis:"
ollama list

# Tentar baixar modelo mais leve (se disponÃ­vel)
echo "ğŸ“¥ Tentando baixar modelo mais leve..."
if ollama pull llama3.1:1b 2>/dev/null; then
    echo "âœ… Modelo llama3.1:1b instalado!"
elif ollama pull llama3.1:3b 2>/dev/null; then
    echo "âœ… Modelo llama3.1:3b instalado!"
else
    echo "âš ï¸ Usando modelo disponÃ­vel (llama3.1:8b) com configuraÃ§Ãµes NITRO"
fi

# Configurar variÃ¡veis de ambiente
echo "ğŸ”§ Configurando variÃ¡veis de ambiente..."
echo 'export LLM_MODEL="llama3.1:8b"' >> ~/.bashrc
echo 'export OLLAMA_BASE="http://localhost:11434/v1"' >> ~/.bashrc

echo ""
echo "ğŸ‰ ConfiguraÃ§Ã£o NITRO completa!"
echo ""
echo "ğŸ“‹ Para usar:"
echo "   source ~/.bashrc"
echo "   ollama serve"
echo "   python3 app/tests/test_hybrid_system.py"
echo ""
echo "ğŸ’¡ Sistema NITRO otimiza qualquer modelo disponÃ­vel"
echo "ğŸš€ Funciona offline como reforÃ§o para Cursor AI"
echo "âš¡ ConfiguraÃ§Ãµes otimizadas: max_tokens=1024, temperature=0.1"
